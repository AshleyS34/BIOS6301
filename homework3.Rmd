---
title: "homework3"
author: "Ashley Spirrison"
date: "2023-10-01"
output: github_document
---

### Question 1-Writing a simulation and finding power when n=100 patients and 1000 patients respectively.



```{r}
set.seed(123)

n_simulations <- 1000 #repeating procedure 1000 times 
alpha <- 0.05
treatment_effect <- 5
mean_outcome <- 60
sd_outcome <- 20

sig_results <- 0
for (i in 1:n_simulations) {
  treatment_group <- sample(0:1, size=100, replace = TRUE) #for sample size of 100 patients 
  outcome <- rnorm(100,mean_outcome, sd_outcome)
  outcome[treatment_group == 1] <- outcome[treatment_group == 1] + treatment_effect
  data <-data.frame(Treatment = factor(treatment_group), Outcome =outcome)
  model <- lm(Outcome ~ Treatment, data = data)
  p_value <- summary(model)$coefficients[2, "Pr(>|t|)"]
  if (p_value <= alpha) { 
    sig_results <- sig_results + 1
    }
}
power <- sig_results/ n_simulations
power
 

```
```{r}
set.seed(123)

n_simulations <- 1000 #repeating procedure 1000 times 
alpha <- 0.05
treatment_effect <- 5
mean_outcome <- 60
sd_outcome <- 20

sig_results <- 0
for (i in 1:n_simulations) {
  treatment_group <- sample(0:1, size=1000, replace = TRUE) #for sample size of 1000 patients 
  outcome <- rnorm(100,mean_outcome, sd_outcome)
  outcome[treatment_group == 1] <- outcome[treatment_group == 1] + treatment_effect
  data <-data.frame(Treatment = factor(treatment_group), Outcome =outcome)
  model <- lm(Outcome ~ Treatment, data = data)
  p_value <- summary(model)$coefficients[2, "Pr(>|t|)"]
  if (p_value <= alpha) { 
    sig_results <- sig_results + 1
    }
}
power <- sig_results/ n_simulations
power
 

```

### Question 2
## showing correlation matrix of wr23 from football values lecture.



    ```{r}
     file <- 'proj_wr23.csv'
    wr = read.csv(file)
    getwd()
    wr
    new.df <- wr[-c(1,2)]
    new.df
    cor.matrix <- cor(new.df)
    cor.matrix

    ```

## Generating a data set with 30 rows that has a similar correlation structure to the previous section and Repeating the procedure 1,000 times. Determining the mean correlation matrix.
```{r}
library(MASS)

  vcov.wr=var(new.df)
vcov.wr
means.wr=colMeans(new.df)
means.wr

n <- 1000
replicate (n,{
wr.sim = mvrnorm(30, mu = means.wr, Sigma = vcov.wr) 
wr.sim = as.data.frame(wr.sim) 
wr.sim=cor(wr.sim)
wr.sim
})

```



### Question 3-running provided code



```{r}
    nDist <- function(n = 100) {
    df <- 10
    prob <- 1/3
    shape <- 1
    size <- 16
    list(
        beta = rbeta(n, shape1 = 5, shape2 = 45),
        binomial = rbinom(n, size, prob),
        chisquared = rchisq(n, df),
        exponential = rexp(n),
        f = rf(n, df1 = 11, df2 = 17),
        gamma = rgamma(n, shape),
        geometric = rgeom(n, prob),
        hypergeometric = rhyper(n, m = 50, n = 100, k = 8),
        lognormal = rlnorm(n),
        negbinomial = rnbinom(n, size, prob),
        normal = rnorm(n),
        poisson = rpois(n, lambda = 25),
        t = rt(n, df),
        uniform = runif(n),
        weibull = rweibull(n, shape)
    )
}
nDist 
```

# Indicating what the code does: 
## This code is showing a function (with the arguement/ sample size/ number of observerations of n =100) being applied to a list of multiple different types of distributions in r and storing in nDist. Additionally, the df, probability, shape, and size are being held constant between the different elements in the list. 


    ```{r}
    rou <- round(sapply(nDist(500), mean), 2)
    rou
    ```

## This code is using sapply to apply a mean function to up to 500 elements of nDist for each of the distribution types in the list. The round and 2 indicate it is rounding the answer to 2 decimal places. It is returning the rounded mean for each distribution type.  


    ```{r}
    sort(apply(replicate(20, round(sapply(nDist(10000), mean), 2)), 1, sd))
    ```

## This code is first applying the mean function to an n of 10,000 for our nDist dataset.The value is then being rounded to two decimal places. The process is next being replicated 20 times. Following this, the sd is applied with a margin of 1 to the dataset that contains our 20 replicates of the rounded data of the mean applied to nDist. Finally, sort is being used to sort a vector based on its values.  


    In the output above, a small value would indicate that `N=10,000` would provide a sufficent sample size as to estimate the mean of the distribution. Let's say that a value *less than 0.02* is "close enough".

3.  For each distribution, estimate the sample size required to simulate the distribution's mean. (15 points)

Don't worry about being exact. It should already be clear that N \< 10,000 for many of the distributions. You don't have to show your work. Put your answer to the right of the vertical bars (`|`) below.


```

| distribution   | N    |Outputs:
|----------------|----- |
| beta           | 10K  | 0
| binomial       | 10K  |0.017
| chisquared     | 20K  |0.04
| exponential    | 10K  |0.011
| f              | 10K  |0.007
| gamma          | 10K  |0.010 
| geometric      | 10K  |0.026
| hypergeometric | 10K  |0.01
| lognormal      | 12K  |0.027
| negbinomial    | 20K  |0.09
| normal         | 10K  |0.011
| poisson        | 20K  |0.05
| t              | 10K  |0.011
| uniform        | 10K  |0.003
| weibull        | 10K |0.013
